{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhbDHXjEgKB4CceawRBZFu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielreinaux/ETL_Spark/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto de ETL e Estruturação de Dados com Apache Spark\n"
      ],
      "metadata": {
        "id": "kZJxKJyNqKIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introdução:\n",
        "\n",
        "Esse projeto foi desenvolvido como parte de um desafio em um processo de ETL. O foco do projeto é o tratamento de dados históricos de precipitação disponibilizados pelo Instituto Nacional de Meteorologia (INMET) no site https://portal.inmet.gov.br/dadoshistoricos.\n",
        "\n",
        "### Demandas e Metodologia:\n",
        "\n",
        "1. Extração Automatizada: A extração dos dados foi realizada de forma automatizada utilizando a linguagem Python e bibliotecas adequadas para manipulação de arquivos ZIP e CSV.\n",
        "\n",
        "2. Transformação e Carga com PySpark: As transformações e cargas dos dados foram realizadas utilizando o PySpark, com a criação de tabelas virtuais (spark.sql). Foi utilizada a arquitetura de medalhões, onde:\n",
        "  - Camada Bronze: Armazena os dados brutos extraídos dos arquivos ZIP.\n",
        "  - Camada Prata: Contém os dados tratados e prontos para consumo."
      ],
      "metadata": {
        "id": "iGFjmZZkDERp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Inicializando o Spark"
      ],
      "metadata": {
        "id": "wfjg6XjjD7CG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "C8Sb6yTsDh4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "NBRZbQ7EqOgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Camada de Bronze\n",
        "\n",
        "1. Coleta Automatizada de Dados: Acessamos o site do INMET para buscar e fazer download dos arquivos ZIP correspondentes aos anos 2000-2004, 2010-2014, e 2020-2024\n",
        "2. Extração dos Arquivos: Os arquivos ZIP são extraídos para diretórios temporários, onde o conteúdo será organizado\n",
        "3. Renomeação e Organização: Os arquivos CSV extraídos são renomeados de acordo com um padrão que facilita a identificação dos dados (Estado, Cidade, Ano) e então movidos para um diretório final, mantendo a estrutura de organização para as próximas etapas"
      ],
      "metadata": {
        "id": "N4IjAauexa1D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XonawnueNHr1"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import re\n",
        "import shutil\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"https://portal.inmet.gov.br/dadoshistoricos\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, como Gecko) Chrome/92.0.4515.131 Safari/537.36\"\n",
        "}\n",
        "# Diretórios dentro do Colab\n",
        "ZIP_DIR = \"/content/zips\"\n",
        "EXTRACT_DIR_BASE = os.path.join(ZIP_DIR, \"extracted\")\n",
        "TEMP_DIR = os.path.join(ZIP_DIR, \"temp\")\n",
        "\n",
        "# Anos desejados\n",
        "ANOS_DESEJADOS = [str(ano) for ano in range(2000, 2005)] + \\\n",
        "                 [str(ano) for ano in range(2010, 2015)] + \\\n",
        "                 [str(ano) for ano in range(2020, 2025)]\n",
        "\n",
        "# Certifique-se de criar os diretórios\n",
        "os.makedirs(ZIP_DIR, exist_ok=True)\n",
        "os.makedirs(EXTRACT_DIR_BASE, exist_ok=True)\n",
        "os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "\n",
        "def fetch_links(base_url, headers, anos_desejados):\n",
        "    \"\"\"Busca os links dos arquivos ZIP para os anos desejados.\"\"\"\n",
        "    response = requests.get(base_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    links = []\n",
        "    for ano in anos_desejados:\n",
        "        link = soup.find('a', string=re.compile(f\"ANO {ano}.*AUTOMÁTICA\"))\n",
        "        if link:\n",
        "            links.append(link['href'])\n",
        "    return links\n",
        "\n",
        "def download_zip(link, zip_dir):\n",
        "    \"\"\"Faz o download do arquivo ZIP a partir de um link e retorna o caminho local.\"\"\"\n",
        "    filename = os.path.basename(link)\n",
        "    zip_path = os.path.join(zip_dir, filename)\n",
        "\n",
        "    response = requests.get(link)\n",
        "    with open(zip_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    return zip_path\n",
        "\n",
        "def extract_zip(zip_path, extract_dir, temp_dir):\n",
        "    \"\"\"Extrai o conteúdo do ZIP para um diretório temporário e retorna o diretório extraído.\"\"\"\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "\n",
        "    return os.listdir(temp_dir)\n",
        "\n",
        "def renomear_e_mover_csv(file, source_folder, target_folder):\n",
        "    \"\"\"Renomeia o arquivo CSV e move para o diretório final.\"\"\"\n",
        "    if file.endswith('.CSV'):\n",
        "        state = file.split('_')[2]\n",
        "        city = file.split('_')[4]\n",
        "        year = file.split('_')[5][-4:]\n",
        "        new_name = f\"{state}_{city}_{year}.csv\"\n",
        "        final_path = os.path.join(target_folder, new_name)\n",
        "        shutil.move(os.path.join(source_folder, file), final_path)\n",
        "\n",
        "def process_zip(zip_path, extract_dir, temp_dir):\n",
        "    \"\"\"Processa um arquivo ZIP baixado: extrai, renomeia e move arquivos CSV.\"\"\"\n",
        "    extracted_items = extract_zip(zip_path, extract_dir, temp_dir)\n",
        "\n",
        "    if len(extracted_items) == 1 and os.path.isdir(os.path.join(temp_dir, extracted_items[0])):\n",
        "        main_folder = os.path.join(temp_dir, extracted_items[0])\n",
        "        for item in os.listdir(main_folder):\n",
        "            renomear_e_mover_csv(item, main_folder, extract_dir)\n",
        "    else:\n",
        "        for item in extracted_items:\n",
        "            renomear_e_mover_csv(item, temp_dir, extract_dir)\n",
        "\n",
        "    shutil.rmtree(temp_dir)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Função principal para orquestrar o processo de ETL.\"\"\"\n",
        "    links = fetch_links(BASE_URL, HEADERS, ANOS_DESEJADOS)\n",
        "\n",
        "    for link in links:\n",
        "        try:\n",
        "            zip_path = download_zip(link, ZIP_DIR)\n",
        "            ano = os.path.basename(zip_path).split('.')[0]\n",
        "            extract_dir = os.path.join(EXTRACT_DIR_BASE, ano)\n",
        "            if not os.path.exists(extract_dir):\n",
        "                os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "            process_zip(zip_path, extract_dir, TEMP_DIR)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar {link}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  4. Camada de prata\n",
        "\n",
        "Continuei aqui o processo de ETL avançando para a Camada de Prata, onde os dados brutos, já extraídos e organizados na Camada de Bronze, serão transformados e preparados para consumo final. A primeira parte da camanda de prata envolveu:\n",
        "\n",
        "1. Normalização da colunas: Os nomes das colunas são normalizados para remover espaços, pontuações, e caracteres especiais, garantindo uniformidade e compatibilidade\n",
        "\n",
        "2. Extração de Metadados e Colunas: A partir das primeiras linhas de cada arquivo CSV, extraímos os metadados e os nomes das colunas de dados, que serão usados para estruturar o DataFrame final.\n",
        "\n",
        "3. Criação de DataFrames: Com os dados processados, criamos DataFrames estruturados, que incluem tanto os dados variáveis quanto as colunas fixas extraídas dos metadados.\n",
        "\n",
        "4. Salvamento dos Dados: Os DataFrames são consolidados e salvos em arquivos CSV"
      ],
      "metadata": {
        "id": "9bXoWggdxk-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit, when, col\n",
        "import glob\n",
        "import time\n",
        "from functools import reduce\n",
        "\n",
        "def initialize_output_directory(output_dir):\n",
        "    \"Função que cria o diretório de saída\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def normalizar_colunas(nome):\n",
        "    \"Função para normalizar os nomes das colunas\"\n",
        "    return nome.replace(\" \", \"_\").replace(\":\", \"\").replace(\".\", \"\").replace(\",\", \"\").strip()\n",
        "\n",
        "def initialize_spark_session(app_name=\"Processamento CSV\"):\n",
        "    \"Função para inicializar a sessão do Spark\"\n",
        "    return SparkSession.builder.appName(app_name).getOrCreate()\n",
        "\n",
        "def extract_metadata(rdd):\n",
        "    \"Função para extrair metadados das 8 primeiras linhas dos arquivos CSVs\"\n",
        "    metadados = rdd.take(8)\n",
        "    colunas_fixas = [normalizar_colunas(linha.split(\";\")[0].strip()) for linha in metadados]\n",
        "    valores_fixos = [linha.split(\";\")[1].strip() for linha in metadados]\n",
        "    return colunas_fixas, valores_fixos\n",
        "\n",
        "def extract_data_columns(rdd):\n",
        "    \"Função que busca o nome do resto das colunas, presentes na linha 9\"\n",
        "    linha_9 = rdd.take(9)[8]\n",
        "    colunas_dados = [normalizar_colunas(col.strip()) for col in linha_9.split(\";\") if col.strip()]\n",
        "    return colunas_dados\n",
        "\n",
        "def process_csv(file_path, spark):\n",
        "    \"Função para processar um único arquivo CSV\"\n",
        "    rdd = spark.sparkContext.textFile(file_path)\n",
        "    colunas_fixas, valores_fixos = extract_metadata(rdd)\n",
        "    colunas_dados = extract_data_columns(rdd)\n",
        "    rdd_dados = rdd.zipWithIndex().filter(lambda x: x[1] >= 9).keys()\n",
        "    df_dados = rdd_dados.map(lambda line: line.split(\";\")).toDF(colunas_dados)\n",
        "\n",
        "    for i, col in enumerate(colunas_fixas):\n",
        "        df_dados = df_dados.withColumn(col, lit(valores_fixos[i]))\n",
        "\n",
        "    df_dados = df_dados.select(colunas_fixas + colunas_dados)\n",
        "\n",
        "    return df_dados\n",
        "\n",
        "def save_dataframe_to_csv(df, output_dir, ano):\n",
        "    \"Função para salvar o DataFrame processado em um arquivo CSV\"\n",
        "    output_file_dir = os.path.join(output_dir, f\"{ano}_prata\")\n",
        "    df.coalesce(1).write.option(\"header\", \"true\") \\\n",
        "        .option(\"delimiter\", \";\") \\\n",
        "        .option(\"encoding\", \"UTF-8\") \\\n",
        "        .csv(output_file_dir)\n",
        "\n",
        "    generated_csv = glob.glob(output_file_dir + \"/part-*.csv\")[0]\n",
        "    final_csv = os.path.join(output_dir, f\"{ano}_prata.csv\")\n",
        "    shutil.move(generated_csv, final_csv)\n",
        "\n",
        "    shutil.rmtree(output_file_dir)\n",
        "\n",
        "def process_all_csvs_by_year(base_dir, output_base_dir, spark):\n",
        "    \"Função para processar todos os arquivos CSV por ano\"\n",
        "    initialize_output_directory(output_base_dir)\n",
        "    start_time_total = time.time()\n",
        "    anos = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
        "\n",
        "    for ano in anos:\n",
        "        start_time = time.time()\n",
        "        bronze_dir = os.path.join(base_dir, ano)\n",
        "        all_files = glob.glob(os.path.join(bronze_dir, \"*.csv\"))\n",
        "\n",
        "        dataframes = []\n",
        "\n",
        "        print(f'Iniciando o processamento para o ano: {ano}')\n",
        "\n",
        "        for file_path in all_files:\n",
        "            df_dados = process_csv(file_path, spark)\n",
        "            dataframes.append(df_dados)\n",
        "\n",
        "        df_final = reduce(lambda df1, df2: df1.unionAll(df2), dataframes)\n",
        "        save_dataframe_to_csv(df_final, output_base_dir, ano)\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"Processamento do ano {ano} concluído em {total_time:.2f} segundos.\")\n",
        "\n",
        "    total_time_total = time.time() - start_time_total\n",
        "    print(f\"Tempo total para processar todos os anos: {total_time_total:.2f} segundos\")\n"
      ],
      "metadata": {
        "id": "_nqJ979-r1vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função principal para executar o processo\n",
        "def main():\n",
        "    \"Função principal\"\n",
        "    bronze_dir = os.path.join(ZIP_DIR, \"extracted\")\n",
        "    prata_dir = \"/content/prata\"\n",
        "    spark = initialize_spark_session(\"Processamento CSV\")\n",
        "    process_all_csvs_by_year(bronze_dir, prata_dir, spark)\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1MlAUrU-o0W",
        "outputId": "2b33ed24-f721-4057-f6fb-1dc087507fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando o processamento para o ano: 2000\n",
            "Processamento do ano 2000 concluído em 4.47 segundos.\n",
            "Iniciando o processamento para o ano: 2001\n",
            "Processamento do ano 2001 concluído em 15.63 segundos.\n",
            "Iniciando o processamento para o ano: 2002\n",
            "Processamento do ano 2002 concluído em 34.47 segundos.\n",
            "Iniciando o processamento para o ano: 2003\n",
            "Processamento do ano 2003 concluído em 62.29 segundos.\n",
            "Iniciando o processamento para o ano: 2004\n",
            "Processamento do ano 2004 concluído em 67.36 segundos.\n",
            "Iniciando o processamento para o ano: 2010\n",
            "Processamento do ano 2010 concluído em 617.94 segundos.\n",
            "Iniciando o processamento para o ano: 2011\n",
            "Processamento do ano 2011 concluído em 693.65 segundos.\n",
            "Iniciando o processamento para o ano: 2012\n",
            "Processamento do ano 2012 concluído em 754.75 segundos.\n",
            "Iniciando o processamento para o ano: 2013\n",
            "Processamento do ano 2013 concluído em 793.35 segundos.\n",
            "Iniciando o processamento para o ano: 2014\n",
            "Processamento do ano 2014 concluído em 823.86 segundos.\n",
            "Iniciando o processamento para o ano: 2020\n",
            "Processamento do ano 2020 concluído em 1075.28 segundos.\n",
            "Iniciando o processamento para o ano: 2021\n",
            "Processamento do ano 2021 concluído em 1094.89 segundos.\n",
            "Iniciando o processamento para o ano: 2022\n",
            "Processamento do ano 2022 concluído em 1067.89 segundos.\n",
            "Iniciando o processamento para o ano: 2023\n",
            "Processamento do ano 2023 concluído em 1098.19 segundos.\n",
            "Iniciando o processamento para o ano: 2024\n",
            "Processamento do ano 2024 concluído em 903.85 segundos.\n",
            "Tempo total para processar todos os anos: 9107.91 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Continuação da Camada de Prata\n",
        "\n",
        "Na segunda parte da Camada de Prata, realizamos o refinamento final dos dados processados. Nesta etapa, nos concentramos em três principais ações:\n",
        "\n",
        "1. Normalização de Cabeçalhos e Valores, removendo colunas especiais geradas pela não decodificação\n",
        "2. Tratamento de Valores Nulos: Substituindo valores -999 por valores nulos de fato ('null')\n",
        "3. Combinação de DataFrames: Consolidar os dados dos arquivos separados em anos, em um único CSV, facilitando a manipulação e análise subsequente."
      ],
      "metadata": {
        "id": "9kwmzVWpdonK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o cabeçalho desejado fora do fluxo principal do código\n",
        "new_header = [\n",
        "    'REGIAO', 'UF', 'ESTACAO', 'CODIGO_(WMO)', 'LATITUDE', 'LONGITUDE',\n",
        "    'ALTITUDE', 'DATA_DE_FUNDACAO_(YYYY-MM-DD)', 'DATA_(YYYY-MM-DD)', 'HORA_(UTC)', 'PRECIPITACAO_TOTAL',\n",
        "    'HORARIO_(mm)', 'PRESSAO_ATMOSFERICA_AO_NIVEL_DA_ESTACAO_HORARIA_(mB)',\n",
        "    'PRESSAO_ATMOSFERICA_MAX_NA_HORA_ANT_(AUT)_(mB)', 'PRESSAO_ATMOSFERICA_MIN_NA_HORA_ANT_(AUT)_(mB)',\n",
        "    'RADIACAO_GLOBAL_(KJ/m_2)', 'TEMPERATURA_DO_AR_-_BULBO_SECO_HORARIA_(°C)', 'TEMPERATURA_DO_PONTO_DE_ORVALHO_(°C)',\n",
        "    'TEMPERATURA_MAXIMA_NA_HORA_ANT_(AUT)_(°C)', 'TEMPERATURA_MINIMA_NA_HORA_ANT_(AUT)_(°C)',\n",
        "    'TEMPERATURA_ORVALHO_MAX_NA_HORA_ANT_(AUT)_(°C)', 'TEMPERATURA_ORVALHO_MIN_NA_HORA_ANT_(AUT)_(°C)',\n",
        "    'UMIDADE_REL_MAX_NA_HORA_ANT_(AUT)_(%)', 'UMIDADE_REL_MIN_NA_HORA_ANT_(AUT)_(%)',\n",
        "    'UMIDADE_RELATIVA_DO_AR_HORARIA_(%)', 'VENTO_DIRECAO_HORARIA_(gr)_(°_(gr))',\n",
        "    'VENTO_RAJADA_MAXIMA_(m/s)', 'VENTO_VELOCIDADE_HORARIA_(m/s)'\n",
        "]\n",
        "\n",
        "def normalizar_cabecalhos_e_valores(df):\n",
        "    \"\"\"\n",
        "    Normaliza os cabeçalhos e valores das colunas, removendo caracteres especiais.\n",
        "    \"\"\"\n",
        "    for col in df.columns:\n",
        "        df = df.withColumnRenamed(col, col.encode('ascii', 'ignore').decode('ascii'))\n",
        "    return df\n",
        "\n",
        "def tratar_valores_nulos(df):\n",
        "    \"\"\"\n",
        "    Substitui valores -999 por null no DataFrame.\n",
        "    \"\"\"\n",
        "    df = df.select([when(col(c) == -999, None).otherwise(col(c)).alias(c) for c in df.columns])\n",
        "    return df\n",
        "\n",
        "def carregar_e_normalizar_csvs(prata_dir):\n",
        "    \"\"\"\n",
        "    Carrega todos os arquivos CSV do diretório fornecido, normaliza os cabeçalhos e os valores,\n",
        "    substitui valores -999 por null, e retorna uma lista de DataFrames.\n",
        "    \"\"\"\n",
        "    all_csvs = glob.glob(os.path.join(prata_dir, \"*.csv\"))\n",
        "    dataframes = []\n",
        "\n",
        "    for csv_file in all_csvs:\n",
        "        df = spark.read.option(\"header\", \"true\").option(\"sep\", \";\").option(\"encoding\", \"UTF-8\").csv(csv_file)\n",
        "        df = normalizar_cabecalhos_e_valores(df)\n",
        "        df = tratar_valores_nulos(df)\n",
        "        dataframes.append(df)\n",
        "\n",
        "    return dataframes\n",
        "\n",
        "def combinar_dataframes(dataframes):\n",
        "    \"\"\"\n",
        "    Combina todos os DataFrames em um único DataFrame usando a operação de união (union).\n",
        "    \"\"\"\n",
        "    df_final = dataframes[0]\n",
        "    for df in dataframes[1:]:\n",
        "        df_final = df_final.union(df)\n",
        "\n",
        "    return df_final\n",
        "\n",
        "def renomear_cabecalhos(df_final, new_header):\n",
        "    \"\"\"\n",
        "    Renomeia os cabeçalhos do DataFrame final com base na lista fornecida em new_header.\n",
        "    \"\"\"\n",
        "    for old_col, new_col in zip(df_final.columns, new_header):\n",
        "        df_final = df_final.withColumnRenamed(old_col, new_col)\n",
        "    return df_final\n",
        "\n",
        "def salvar_dataframe(df_final, output_dir, output_filename):\n",
        "    \"\"\"\n",
        "    Salva o DataFrame final em um arquivo CSV, movendo-o para fora do diretório temporário\n",
        "    e removendo o diretório temporário após a operação.\n",
        "    \"\"\"\n",
        "    output_temp_dir = os.path.join(output_dir, \"temp_output\")\n",
        "    df_final.coalesce(1).write.option(\"header\", \"true\").option(\"sep\", \";\").option(\"encoding\", \"UTF-8\").csv(output_temp_dir)\n",
        "\n",
        "    generated_csv = glob.glob(output_temp_dir + \"/part-*.csv\")[0]\n",
        "    final_csv = os.path.join(output_dir, output_filename)\n",
        "    shutil.move(generated_csv, final_csv)\n",
        "\n",
        "    shutil.rmtree(output_temp_dir)\n",
        "\n",
        "    return final_csv\n",
        "\n",
        "def processar_csvs(prata_dir, new_header, output_filename=\"dataset_final.csv\"):\n",
        "    \"\"\"\n",
        "    Processa todos os arquivos CSV no diretório fornecido, combinando-os em um único CSV final,\n",
        "    substituindo valores -999 por null e renomeando os cabeçalhos das colunas com base na lista new_header.\n",
        "    \"\"\"\n",
        "    # Carregando e normalizando os CSVs\n",
        "    dataframes = carregar_e_normalizar_csvs(prata_dir)\n",
        "\n",
        "    # Combinando os DataFrames em um DataFrame final\n",
        "    df_final = combinar_dataframes(dataframes)\n",
        "\n",
        "    # Fazendo ajustes na coluna\n",
        "    df_final = renomear_cabecalhos(df_final, new_header)\n",
        "\n",
        "    # Salvando o DataFrame final\n",
        "    final_csv = salvar_dataframe(df_final, prata_dir, output_filename)\n",
        "\n",
        "    print(f\"Todos os arquivos CSV foram combinados e salvos em um único arquivo CSV: {final_csv}\")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Juntar CSVs\").getOrCreate()\n",
        "\n",
        "prata_dir = \"/content/prata\"\n",
        "\n",
        "processar_csvs(prata_dir, new_header)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr1PP3ly9ZLV",
        "outputId": "8f46ed6d-19d1-4eb2-efcb-0eef7660f999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todos os arquivos CSV foram combinados e salvos em um único arquivo CSV: /content/prata/dataset_final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo do DataFrame final"
      ],
      "metadata": {
        "id": "ZlpNxY7BGuzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Visualizar Dataset\").getOrCreate()\n",
        "csv_path = \"/content/prata/dataset_final.csv\"\n",
        "df = spark.read.option(\"header\", \"true\").option(\"sep\", \";\").option(\"encoding\", \"UTF-8\").csv(csv_path)\n",
        "df.show(15)\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVky2K9zZQIc",
        "outputId": "28e8ac3f-8929-4caa-a20d-5258121c14e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+---------------+------------+-----------+------------+--------+-----------------------------+-----------------+----------+------------------+------------+----------------------------------------------------+----------------------------------------------+----------------------------------------------+------------------------+-------------------------------------------+------------------------------------+-----------------------------------------+-----------------------------------------+----------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+----------------------------------+-----------------------------------+-------------------------+\n",
            "|REGIAO| UF|        ESTACAO|CODIGO_(WMO)|   LATITUDE|   LONGITUDE|ALTITUDE|DATA_DE_FUNDACAO_(YYYY-MM-DD)|DATA_(YYYY-MM-DD)|HORA_(UTC)|PRECIPITACAO_TOTAL|HORARIO_(mm)|PRESSAO_ATMOSFERICA_AO_NIVEL_DA_ESTACAO_HORARIA_(mB)|PRESSAO_ATMOSFERICA_MAX_NA_HORA_ANT_(AUT)_(mB)|PRESSAO_ATMOSFERICA_MIN_NA_HORA_ANT_(AUT)_(mB)|RADIACAO_GLOBAL_(KJ/m_2)|TEMPERATURA_DO_AR_-_BULBO_SECO_HORARIA_(°C)|TEMPERATURA_DO_PONTO_DE_ORVALHO_(°C)|TEMPERATURA_MAXIMA_NA_HORA_ANT_(AUT)_(°C)|TEMPERATURA_MINIMA_NA_HORA_ANT_(AUT)_(°C)|TEMPERATURA_ORVALHO_MAX_NA_HORA_ANT_(AUT)_(°C)|TEMPERATURA_ORVALHO_MIN_NA_HORA_ANT_(AUT)_(°C)|UMIDADE_REL_MAX_NA_HORA_ANT_(AUT)_(%)|UMIDADE_REL_MIN_NA_HORA_ANT_(AUT)_(%)|UMIDADE_RELATIVA_DO_AR_HORARIA_(%)|VENTO_DIRECAO_HORARIA_(gr)_(°_(gr))|VENTO_RAJADA_MAXIMA_(m/s)|\n",
            "+------+---+---------------+------------+-----------+------------+--------+-----------------------------+-----------------+----------+------------------+------------+----------------------------------------------------+----------------------------------------------+----------------------------------------------+------------------------+-------------------------------------------+------------------------------------+-----------------------------------------+-----------------------------------------+----------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+----------------------------------+-----------------------------------+-------------------------+\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0000 UTC|                 0|       911,5|                                               911,6|                                         910,9|                                          null|                    17,8|                                       null|                                17,9|                                     17,2|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0100 UTC|                 0|       911,9|                                               911,9|                                         911,5|                                          null|                    18,2|                                       null|                                18,3|                                     17,6|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0200 UTC|                 0|       911,6|                                               911,9|                                         911,5|                                          null|                    17,7|                                       null|                                18,3|                                     17,6|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0300 UTC|                 0|       910,9|                                               911,5|                                         910,9|                                          null|                    17,1|                                       null|                                17,7|                                     16,8|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0400 UTC|                 0|       910,3|                                               910,9|                                         910,3|                                          null|                    16,8|                                       null|                                17,4|                                     16,5|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0500 UTC|                 0|       909,6|                                               910,3|                                         909,6|                                          null|                      16|                                       null|                                17,4|                                     15,8|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0600 UTC|                 0|       909,4|                                               909,6|                                         909,3|                                          null|                    15,5|                                       null|                                16,1|                                     15,5|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0700 UTC|                 0|       909,2|                                               909,4|                                         909,2|                                          null|                    16,8|                                       null|                                16,9|                                       15|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0800 UTC|                 0|       909,3|                                               909,4|                                         909,1|                                          null|                    16,2|                                       null|                                16,8|                                       15|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  0900 UTC|                 0|       910,1|                                               910,1|                                         909,3|                                          56,6|                      17|                                       null|                                17,1|                                       16|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  1000 UTC|                 0|       910,7|                                               910,7|                                         910,1|                                         215,9|                    19,8|                                       null|                                19,8|                                     16,8|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  1100 UTC|                 0|       911,5|                                               911,5|                                         910,7|                                        1489,7|                    20,4|                                       null|                                21,1|                                     19,5|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  1200 UTC|                 0|       911,6|                                               911,6|                                         911,4|                                        2407,5|                      22|                                       null|                                22,2|                                     20,4|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  1300 UTC|                 0|       911,5|                                               911,6|                                         911,5|                                          3126|                    24,6|                                       null|                                24,8|                                       22|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "|     S| SC|RANCHO QUEIMADO|        A870|-27,6786111|-49,04194444|     881|                     31/05/16|       2022/01/01|  1400 UTC|                 0|       911,5|                                               911,6|                                         911,4|                                        3648,3|                    25,8|                                       null|                                26,6|                                     24,1|                                     null|                                          null|                                          null|                                 null|                                 null|                              null|                               null|                     null|\n",
            "+------+---+---------------+------------+-----------+------------+--------+-----------------------------+-----------------+----------+------------------+------------+----------------------------------------------------+----------------------------------------------+----------------------------------------------+------------------------+-------------------------------------------+------------------------------------+-----------------------------------------+-----------------------------------------+----------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+----------------------------------+-----------------------------------+-------------------------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}